####################################################################
APA Project (FIB - UPC)
Josep de Cid & Gonzalo Recio

Breast Cancer Diagnostic
Q1 2017-2018
####################################################################

First install necessary packages, skip  this step or some installations
if some are already installed.

```{Setup}
install.packages('corrplot')
install.packages('caTools')
install.packages('ggplot2')
install.packages('rpart')
install.packages('randomForest')
install.packages('caret')
install.packages('e1071')
install.packages('lfda')
install.packages('xgboost')
```

####################################################################
SECTION 1: Data Preprocessing
####################################################################

Let's start reading the dataset and removing unnecessary columns.
- id is a useless column for classification
- X is a NaN column

```{R Dataset}
dataset <- read.csv('data.csv')
dataset <- subset(x = dataset,
                  select = -c(X, id))

summary(dataset)
```

Once checked that there are no NA or out of range values, we consider that it's not necessary to deal with missing or invalid data.
We have to convert the dependant variable 'diagnosis' into factors (2 levels).
The diagnosis distribution is very unbalanced, having 357 Benign and 212 Malign observations.
As long as we have numeric variables that are shown in different scales, some methods must need to scale or center to work properly, like KNN, NN...

```{R Preprocessing}
diagnosis <- as.factor(dataset$diagnosis)
dataset$diagnosis <- diagnosis

summary(dataset$diagnosis)
plot(x = diagnosis,
     main = 'Diagnosis distribution',
     xlab = 'Diagnosis',
     ylab = 'Count')

# Feature scaling
dataset[, -1] <- scale(dataset[, -1],
                       scale = TRUE,
                       center = TRUE)
```

Let's look at the variables to check some correlation, and try to remove some unnecessary predictors with high correlation among them.

```{R Correlation}
library(corrplot)
correlation <- cor(dataset[, -1])
corrplot(corr = correlation,
         order = 'hclust',
         tl.col = 'black',
         tl.cex = 0.8)
```

There are some variables with almost a correlation of 1. Let's apply a feature selection removing very correlated variables.

```{R Feature selection}
# area_se, radius_se, perimeter_se -> area_se
dataset$radius_se <- NULL
dataset$perimeter_se <- NULL
# area_mean, radius_mean, perimeter_mean -> area_mean
dataset$radius_mean <- NULL
dataset$perimeter_mean <- NULL
# area_worst, radius_worst, perimeter_worst -> area_worst
dataset$radius_worst <- NULL
dataset$area_worst <- NULL
```

Once the dataset basic preprocessing is ready we proceed to split it into Training and Test set.

```{R Split dataset}
library(caTools)
split = sample.split(Y = dataset$diagnosis, SplitRatio = 0.8)
training.set = subset(dataset, split == TRUE)
test.set = subset(dataset, split == FALSE)
```

Now let's apply feature extraction (PCA) because we can easily reduce dimensionality without losing so much information.
With 10 components we get over 0.95 of variance explained. We would need 14, extra components only to obtain an extra 0.05, so we discard these ones.

```{R PCA}
pca <- prcomp(dataset[, -1])
plot(pca, type = 'l')

summary(pca)

pca.df <- as.data.frame(pca$x)

library(ggplot2)
ggplot(pca.df) +
  geom_point(aes(x = PC1, y = PC2, col = diagnosis)) +
  ggtitle('Diagnosis distribution over first two Principal Components')
```

####################################################################
# SECTION 2: Model Building
####################################################################

```{R Util function}
harmMean <- function(elements) {
  return(length(elements) / sum(1 / elements))
}

getAccuracyReport <- function(truth, pred) {
  (conf <- table(Truth = truth,
                 Pred = pred))
  # Percent of each class and accuracy
  (props <- prop.table(conf, 1))
  accuracy <- sum(diag(conf)) / sum(conf)

  # Results
  print(paste0('Accuracy: ', accuracy))
  print(paste0('Error: ', round(100 * (1 - accuracy), 4), '%'))
  print(paste0('Harmonic mean: ', harmMean(diag(conf))))
}
```

```{R LDA model}
library(MASS)
lda <- lda(formula = diagnosis ~ .,
           data = training.set)

# Create LDA for training set.
training.set.lda <- as.data.frame(predict(lda, training.set))
training.set.lda <- training.set.lda[c(1, 4)]
colnames(training.set.lda) <- c('diagnosis', 'LD1')
# Create LDA for test set.
test.set.lda <- as.data.frame(predict(lda, test.set))
test.set.lda <- test.set.lda[c(1, 4)]
colnames(test.set.lda) <- c('diagnosis', 'LD1')

getAccuracyReport(test.set$diagnosis, test.set.lda$diagnosis)
```

```{R LDA plots}                        
# Display diagnosis over 1D.
ggplot(training.set.lda, aes(x = LD1, y = 0, col = diagnosis)) +
  ggtitle('Diagnosis distribution over LD1 (training)') +
  geom_point(alpha = 0.8)

# Display diagnosis over 2D (Density).
ggplot(training.set.lda, aes(x = LD1, fill = diagnosis)) +
  ggtitle('Diagnosis density over first LD (training)') +
  geom_density(alpha = 0.8)
```

# K-NN ----
```{R KNN}
library(class)
set.seed(22431)

pred.knn <- knn(train = training.set[-1],
                test = test.set[-1],
                cl = training.set$diagnosis,
                prob = TRUE,
                k = 3)

(conf.knn <- table(Truth = test.set$diagnosis,
                  Pred = pred.knn))

# Percent of each class and accuracy
(props <- prop.table(conf.knn, 1))
accuracy <- sum(diag(conf.knn)) / sum(conf.knn)

# Results
paste0('Accuracy: ', accuracy)
paste0('Error: ', round(100 * (1 - accuracy), 4), '%')
paste0('Harmonic mean: ', harm(props[1,1], props[2,2])) 
```

```{R KNN CV}
pred.knn.cv <- knn.cv(train = training.set[-1],
                      cl = training.set$diagnosis,
                      k = 3)

(conf.knn <- table(Truth = training.set$diagnosis,
                   Pred = pred.knn.cv))

# Percent of each class and accuracy
(props <- prop.table(conf.knn, 1))
accuracy <- sum(diag(conf.knn)) / sum(conf.knn)

# Results
paste0('Accuracy: ', accuracy)
paste0('Error: ', round(100 * (1 - accuracy), 4), '%')
paste0('Harmonic mean: ', harm(props[1,1], props[2,2])) 
```

As we have 456 observations in our training set, we might try odd k, roughly equal to the square root of 456. With tow-category outcome, use an odd number eliminates tie vote chances.

```{R KNN k}
neighbours <- seq(1, sqrt(nrow(training.set)), 2)
errors <- matrix(c(neighbours, rep(0, length(neighbours))),
                 nrow = length(neighbours))
colnames(errors) <- c('k', 'LOOCV error')

errors2 <- lapply(X = neighbours, FUN = function(k) {
  set.seed(22413)
  pred.knn.cv <- knn.cv(train = training.set[-1],
                        cl = training.set$diagnosis,
                        k = k)
  
  conf.knn <- table(Truth = training.set$diagnosis,
                   Pred = pred.knn.cv)
  return(1 - sum(diag(conf.knn)) / sum(conf.knn))
})

errors[, 'LOOCV error'] <- unlist(errors2, recursive = TRUE)
errors

lowest.error<- as.integer(which.min(errors[, 'LOOCV error']))
(k.best <- errors[lowest.error, 'k'])

plot(errors, type = 'b')
```

```{R}
set.seed(22413)
pred.knn <- knn(train = training.set[-1],
                test = test.set[-1],
                cl = training.set$diagnosis,
                prob = TRUE,
                k = k.best)

(conf.knn <- table(Truth = test.set$diagnosis,
                  Pred = pred.rf))

# Percent of each class and accuracy
(props <- prop.table(conf.knn, 1))
accuracy <- sum(diag(conf.knn)) / sum(conf.knn)

# Results
paste0('Accuracy: ', accuracy)
paste0('Error: ', round(100 * (1 - accuracy), 4), '%')
paste0('Harmonic mean: ', harm(props[1,1], props[2,2]))
```

```{R Logistic}
classifier.log <- glm(formula = diagnosis ~ .,
                      family = binomial,
                      data = training.set)

classifier.log.aic <- step(classifier.log)

prob.log <- predict(classifier.log,
                    type = 'response',
                    newdata = test.set[-1])
pred.log <- ifelse(prob.log > 0.5, 'M', 'B')

# Logistic Confusion matrix and accuracy.
(conf.log <- table(test.set[, 1], pred.log))
(acc.log <- (conf.log[1, 1] + conf.log[2, 2]) / dim(test.set)[1])
```

```{R}
p.possible <- seq(0.05, 0.95, by = 0.01)

classifier.log <- glm(formula = diagnosis ~ .,
                        family = binomial,
                        data = training.set)

prob.log <- predict(classifier.log,
                    type = 'response',
                    newdata = test.set[-1])

errors <- sapply(X = p.possible, FUN = function(p) {
  conf <- table(Truth = test.set$diagnosis,
                Pred = as.factor(ifelse(prob.log >= p, 'M', 'B')))
  
  return(1 - (2 / sum(1/diag(conf))))
})

min(1 - min(errors))
props <- prop.table(conf.knn, 1)
lowest.error <- as.integer(which.min(errors))
(p.best <- p.possible[lowest.error])

harm <- function (a,b) { 2 / (1/a + 1/b) }
harm(props[1,1], props[2,2])
```

```{R}
library(randomForest)
set.seed(22413)

model.rf <- randomForest(formula = diagnosis ~ .,
                         data = training.set,
                         ntree = 100,
                         proximity = FALSE)
model.rf
```

We get an estimated test error (OOB) of 3.95%. Let's compute the real test error:

```{R}
harm <- function (a,b) { 2 / (1/a + 1/b) }

pred.rf <- predict(model.rf,
                   newdata = test.set,
                   type = 'class')

(conf.rf <- table(Truth = test.set$diagnosis,
                  Pred = pred.rf))

# Percent of each class and accuracy
(props <- prop.table(conf.rf, 1))
(accuracy <- sum(diag(conf.rf)) / sum(conf.rf))

# Real test error
round(100 * (1 - accuracy), 2)
harm(props[1,1], props[2,2])
```

So OOB really works in estimating prediction error and the RF is better than a single tree

However, there still is a big issue in unbalanced classes
one way to deal with this is to include class weights

```{R}
set.seed(22413)
model.rf <- randomForest(formula = diagnosis ~ .,
                         data = training.set,
                         ntree = 100,
                         proximity = FALSE,
                         classwt = c(10, 1))
model.rf
```

helps a little bit, but not much: we get estimated test error (OOB) of 9.86% with a better balance; let's compute the real test error

```{R}
pred.rf2 <- predict (model.rf2, deposit[test.indexes,], type="class")

(ct <- table(Truth=deposit$subscribed[test.indexes], Pred=pred.rf2))

# percent by class
prop.table(ct, 1)
# total percent correct
sum(diag(ct))/sum(ct)
# real test error is 

round(100*(1-sum(diag(ct))/sum(ct)),2)

(F1 <- harm (prop.table(ct,1)[1,1], prop.table(ct,1)[2,2]))
```

Another way is to stratify the sampling in the boostrap resamples

# 'yes' is the less represented class, so we upsample it

```{R}
n.b <- table(training.set$diagnosis)['B']
n.m <- table(training.set$diagnosis)['M']

set.seed(22413)
model.rf <- randomForest(formula = diagnosis ~ .,
                         data = training.set,
                         ntree = 100,
                         proximity = FALSE,
                         strata = training.set$diagnosis,
                         sampsize = c(n.b, n.m))
model.rf
```

which seems to help much more: we get an estimated test error (OOB) of 17%, but with a better balance

# let's compute the real test error:

```{R}
pred.rf <- predict(model.rf,
                   newdata = test.set,
                   type = 'class')

(conf.rf <- table(Truth = test.set$diagnosis,
                  Pred = pred.rf))

# Percent of each class and accuracy
(props <- prop.table(conf.rf, 1))
(accuracy <- sum(diag(conf.rf)) / sum(conf.rf))

# Real test error
round(100 * (1 - accuracy), 2)
harm(props[1,1], props[2,2])

```

Let's optimize the number of trees based on OOB error:

```{R}
(ntrees <- round(10^seq(1,3,by=0.2)))

rf.results <- matrix(c(ntrees, rep(0, length(ntrees))),
                     nrow = length(ntrees))
colnames(rf.results) <- c('nTrees', 'OOB')

OOBs <- lapply(X = ntrees, FUN = function(nt) {
  set.seed(22413)
  model.rf <- randomForest(formula = diagnosis ~ .,
                           data = training.set,
                           ntree = nt,
                           proximity = FALSE, 
                           # sampsize=c(yes=3000, no=3000),
                           strata = training.set$diagnosis)
  return(model.rf$err.rate[[nt, 1]])
})

rf.results[, 'OOB'] <- unlist(OOBs, recursive = TRUE)
rf.results

# Choose best value of 'nTrees'
lowest.OOB.error<- as.integer(which.min(rf.results[, 'OOB']))
(nt.best <- rf.results[lowest.OOB.error, 'nTrees'])
```

we could also try to optimize the number of variables in the same way but, as it was said in the lectures, the default value (square root) works fine in general

Now refit the RF with the best value of 'ntrees'

```{R}
set.seed(22413)
model.rf <- randomForest(formula = diagnosis ~ .,
                         data = training.set,
                         ntree = nt.best,
                         proximity = FALSE,
                         importance = TRUE,
                         # sampsize=c(yes=3000, no=3000),
                         strata = training.set$diagnosis)
model.rf

pred.rf <- predict(model.rf,
                   newdata = test.set,
                   type = 'class')

(conf.rf <- table(Truth = test.set$diagnosis,
                  Pred = pred.rf))

# Percent of each class and accuracy
(props <- prop.table(conf.rf, 1))
(accuracy <- sum(diag(conf.rf)) / sum(conf.rf))

# Real test error
round(100 * (1 - accuracy), 2)
harm(props[1,1], props[2,2])
```



```{R}
# The importance of variables
plot(model.rf)
varImpPlot(model.rf, type = 1)
vars <- importance(model.rf, type = 1)

plot(model.rf, main = 'Error')

legend('topright',
       legend = c('OOB', 'B', 'M'),
       pch = c(1, 1),
       col = c('black', 'red', 'green'))

# What variables are being used in the forest (their counts)
vars <- cbind(vars, varUsed(model.rf, by.tree=FALSE,count = TRUE))
colnames(vars) = c('Importance', 'Count')
vars
```

```{R SVM}
library(e1071)

classifier.svm.linear <- svm(formula = diagnosis ~ .,
                             data = training.set,
                             type = 'C-classification',
                             kernel = 'linear')
classifier.svm.linear

pred.svm.linear <- predict(classifier.svm.linear,
                           newdata = test.set)

(conf.svm.linear <- table(Truth = test.set$diagnosis,
                          Pred = pred.svm.linear))
(acc.svm.line <- mean(pred.svm.linear == test.set$diagnosis))
```

```{R SVM Polynomial}
classifier.svm.polynomial <- svm(formula = diagnosis ~ .,
                                 data = training.set,
                                 type = 'C-classification',
                                 kernel = 'polynomial')
classifier.svm.polynomial

pred.svm.polynomial <- predict(classifier.svm.polynomial,
                               newdata = test.set)

(conf.svm.polynomial <- table(Truth = test.set$diagnosis,
                              Pred = pred.svm.polynomial))
(acc.svm.polynomial <- mean(pred.svm.polynomial == test.set$diagnosis))
```

```{R SVM Gaussian}
classifier.svm.gaussian <- svm(formula = diagnosis ~ .,
                               data = training.set,
                               type = 'C-classification',
                               kernel = 'radial')
classifier.svm.gaussian

pred.svm.gaussian <- predict(classifier.svm.gaussian,
                             newdata = test.set)

(conf.svm.gaussian <- table(Truth = test.set$diagnosis,
                            Pred = pred.svm.gaussian))
(acc.svm.gaussian <- mean(pred.svm.linear == test.set$diagnosis))
```

```{R SVM Sigmoid}
classifier.svm.sigmoid <- svm(formula = diagnosis ~ .,
                              data = training.set,
                              type = 'C-classification',
                              kernel = 'sigmoid')
classifier.svm.sigmoid

pred.svm.sigmoid <- predict(classifier.svm.sigmoid,
                            newdata = test.set)

(conf.svm.linear <- table(Truth = test.set$diagnosis,
                          Pred = pred.svm.sigmoid))
(acc.svm.line <- mean(pred.svm.sigmoid == test.set$diagnosis))
```


```{R Naive Bayes}
library(e1071)

set.seed(22413)
model.nb <- naiveBayes(formula = diagnosis ~ .,
                       data = training.set)
model.nb

predict(model.nb, newdata = test.set[-1])
predict(model.nb, newdata = test.set[-1], type = 'raw')
pred.nb <- predict(model.nb, newdata = training.set[-1])

(conf.rf <- table(Truth = training.set$diagnosis,
                  Pred = pred.nb))

# Percent of each class and accuracy
(props <- prop.table(conf.rf, 1))
(accuracy <- sum(diag(conf.rf)) / sum(conf.rf))

# Real test error
round(100 * (1 - accuracy), 2)
harm(props[1,1], props[2,2])
```

# Neural Networks ----
library(nnet)
classifier.nn <- nnet(formula = diagnosis ~ .,
                      data = training.set,
                      size = 10, maxit = 2000, decay = 0)

pred.nn <- as.factor(predict(classifier.nn, newdata = test.set, type = 'class'))
(conf.nn <- table(pred.nn, test.set$diagnosis))
(acc.nn <- (conf.nn[1, 1] + conf.nn[2, 2]) / dim(test.set)[1])

par(mfrow=c(3,2))
for (i in 1:3)
{
  set.seed(42)
  nn1 <- nnet(formula=diagnosis ~ ., data=training.set, size=i, decay=0, maxit=2000,trace=T)
  pred.nn1 <- as.numeric(as.factor(predict(nn1, type='class')))
 
  pca.tr <- prcomp(training.set[, 2:31])
  pca.tr.df <- as.data.frame(pca.tr$x)
  plot(pca.tr.df$PC2 ~ pca.tr.df$PC1,pch=20,col=c('red','green')[pred.nn1])
  title(main=paste(i,'hidden unit(s)'))
  plot(pca.tr.df$PC2 ~ pca.tr.df$PC1, pch=20,col=c('red','green')[as.numeric(training.set$diagnosis)])
  title(main='Real Diagnosis')
}

par(mfrow=c(3,2))
for (i in 1:3)
{
  set.seed(42)
  nn1 <- nnet(formula=diagnosis ~ ., data=training.set, size=i, decay=0, maxit=2000,trace=T)
  pred.nn1 <- as.numeric(as.factor(predict(nn1, type='class')))

  fda.tr <- lfda(training.set[-1], training.set[1], r = 3, metric='plain')
  fda.tr.df <- as.data.frame(fda.tr$Z)
  plot(fda.tr.df$V3 ~ fda.tr.df$V1, pch=20,col=c('red','green')[pred.nn1])
  title(main=paste(i,'hidden unit(s)'))
  plot(fda.tr.df$V3 ~ fda.tr.df$V1, pch=20,col=c('red','green')[as.numeric(training.set$diagnosis)])
  title(main='Real Diagnosis')
}

par(mfrow=c(1,1))

# With 3 hidden units, que NN learns quite perfectly with normalized data

# This method finds that best number of hidden units is 5 and decay weight value 0.1
set.seed(42)
nnet <- train(form = diagnosis ~ ., data=training.set, method = 'nnet', metric = 'Accuracy', maxit=2000,trace=T, linout = F)
pred.nnet <- as.numeric(as.factor(predict(nnet, newdata = test.set)))
(conf.nnet <- table(pred.nnet, test.set$diagnosis))
(acc.nnet <- (conf.nnet[1, 1] + conf.nnet[2, 2]) / dim(test.set)[1])

#coef(nnet)
train_control <- trainControl(method="LOOCV", number = 10)
grid <- expand.grid(decay = 10^seq(-3, 0, 0.3),
                    size = seq(1, 10))
nnet <- train(form = diagnosis ~.,
              data = training.set,
              method = 'nnet', 
              tuneGrid = grid, 
              trControl = trc,
              trace = FALSE)

pred.nn <- as.factor(predict(nnet, newdata = test.set, type = 'raw'))
(conf.nn <- table(pred.nn, test.set$diagnosis))
(acc.nn <- (conf.nn[1, 1] + conf.nn[2, 2]) / dim(test.set)[1])
# 98.2% accuracy
# Gradient Boosting ----
tune.xgb <- expand.grid(
  eta = c(0.01, 0.001, 0.0001),
  nrounds = 500,
  lambda = 1,
  alpha = 0
)

classifier.xgb <- train(x = as.matrix(training.set[-1]),
                        y = training.set$diagnosis,
                        method = 'xgbLinear',
                        trControl = trc,
                        tuneGrid = tune.xgb)

pred.xgb <- predict(classifier.xgb,
                    newdata = test.set)

# Cost plots, Confusion matrix and accuracy.
(conf.xgb <- confusionMatrix(data = classifier.xgb,
                             reference = test.set$diagnosis,
                             positive = 'M'))
(acc.xgb <- mean(pred.xgb == test.set$diagnosis))

####################################################################
# SECTION 3: Model Comparaison
####################################################################

classifiers <- list(
  KNN = classifier.knn,
  SVM = classifier.svm.poly,
  RF = classifier.rf,
  NN = nnet,
  GB = classifier.xgb)

models.corr <- modelCor(classifiers)